no | result                                    | embed_num | max_len | min_count | lr     | epochs | vocab_size | fclayer
---|-------------------------------------------|-----------|---------|-----------|--------|--------|------------|--------
1  | loss=0.727, testacc=0.777, trainacc=0.737 | 128       | 1024    | 5         | 0.0005 | 200    | 16000      | 1
2  | loss=0.642, testacc=0.799, trainacc=0.774 | 128       | 1024    | 5         | 0.0005 | 200    | 8000       | 1
3  | loss=0.350, testacc=0.826, trainacc=0.844 | 128       | 1024    | 5         | 0.0005 | 200    | 8000       | 2
4  | loss=0.308, testacc=0.802, trainacc=0.854 | 128       | 1024    | 5         | 0.0005 | 200    | 8000       | 3
5  | loss=0.260, testacc=0.803, trainacc=0.863 | 128       | 1024    | 5         | 0.0005 | 300    | 8000       | 3
6  | loss=0.242, testacc=0.819, trainacc=0.885 | 256       | 1024    | 5         | 0.0005 | 200    | 8000       | 2
7  | loss=0.278, testacc=0.810, trainacc=0.883 | 256       | 1024    | 2         | 0.0005 | 200    | 8000       | 2
8  | loss=0.488, testacc=0.836, trainacc=0.797 | 128       | 1024    | 5         | 0.0005 | 100    | 8000       | 2
9  | loss=0.492, testacc=0.840, trainacc=0.803 | 128       | 512     | 5         | 0.0005 | 100    | 8000       | 2
10 | loss=0.551, testacc=0.807, trainacc=0.788 | 128       | 256     | 5         | 0.0005 | 100    | 8000       | 2
11 | loss=0.482, testacc=0.819, trainacc=0.807 | 128       | 512     | 5         | 0.0005 | 100    | 8000       | 3
12 | loss=0.663, testacc=0.812, trainacc=0.755 | 128       | 512     | 5         | 0.0001 | 200    | 8000       | 2



best: no.9, loss=0.492, testacc=0.840, trainacc=0.803
doc2vec.infer의 랜덤성 때문인지 시행할 때마다 성능이 계속 바뀌고, 파라미터에 따른 큰차이는 없다.

훈련을 시킬때는 긴 문장으로 훈련을 시키고 직접 문장을 입력해서 테스트 할때는 짧은 문장으로 테스트를 해서인지 성능이 애매하다.
문장이 길이에 상관없이 고정된 크기의 벡터로 임베딩을 해서 생기는 문제라고 생각된다.

이러한 문제점을 해결한 kobert는 문장의 길이에 상관없이 항상 좋은 성능을 보인다.